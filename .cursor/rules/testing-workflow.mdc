---
description: Testing workflow after implementing plans or features
globs:
alwaysApply: false
---

# Testing Workflow

After implementing a plan or feature, follow this workflow to generate a testing plan, verify with Playwright, fix issues, and write an e2e test.

## Step 1: Generate Testing Plan

After completing implementation, create a testing plan file at:
```
.cursor/testing-plans/{feature-name}.md
```

### Testing Plan Template

```markdown
# Testing Plan: {Feature Name}

## Overview
Brief description of what was implemented and what needs to be tested.

## Prerequisites
- [ ] Development server running on port {port}
- [ ] Database seeded with test data
- [ ] Test user credentials available

## Test Scenarios

### Scenario 1: {Happy Path}
**Description:** {What this scenario tests}
**Steps:**
1. Navigate to {URL}
2. {Action}
3. {Action}
**Expected Result:** {What should happen}

### Scenario 2: {Edge Case}
**Description:** {What this scenario tests}
**Steps:**
1. {Action}
2. {Action}
**Expected Result:** {What should happen}

### Scenario 3: {Error Handling}
**Description:** {What this scenario tests}
**Steps:**
1. {Action}
2. {Action}
**Expected Result:** {Error message or validation}

## UI Elements to Verify
- [ ] {Element} renders correctly
- [ ] {Element} is interactive
- [ ] {Loading state} displays properly
- [ ] {Empty state} displays when no data

## API/Data Verification
- [ ] {tRPC route} returns expected data
- [ ] {Mutation} updates database correctly
- [ ] Error states handled properly

## Accessibility Checks
- [ ] Keyboard navigation works
- [ ] Focus states visible
- [ ] ARIA labels present
```

## Step 2: Manual Verification with Playwright MCP

Use the browser MCP tools to manually verify each scenario:

### Navigation & Snapshot Pattern
```
1. browser_navigate → Navigate to the page
2. browser_snapshot → Get accessibility tree of elements
3. browser_click/type → Interact with elements using refs from snapshot
4. browser_snapshot → Verify state changed
5. browser_take_screenshot → Visual verification if needed
```

### Example Verification Flow
```typescript
// 1. Navigate to the feature
mcp_cursor-ide-browser_browser_navigate({ url: "http://localhost:5173/path" })

// 2. Get page state
mcp_cursor-ide-browser_browser_snapshot()

// 3. Interact with elements (use ref from snapshot)
mcp_cursor-ide-browser_browser_click({ element: "Button description", ref: "ref-from-snapshot" })

// 4. Verify result
mcp_cursor-ide-browser_browser_snapshot()
```

### Common Verification Patterns

**Form Submission:**
1. Navigate to form page
2. Snapshot to get form field refs
3. Type into each field
4. Click submit button
5. Snapshot to verify success state

**Data Table:**
1. Navigate to table page
2. Snapshot to verify rows render
3. Test search/filter by typing
4. Snapshot to verify filtered results
5. Test pagination if applicable

**Modal Flow:**
1. Snapshot to get trigger button ref
2. Click trigger to open modal
3. Snapshot to verify modal content
4. Fill modal form if applicable
5. Click action button
6. Snapshot to verify modal closed and result

## Step 3: Fix Issues Found

When verification reveals issues:

1. **Document the issue** in the testing plan with:
   - What was expected
   - What actually happened
   - Screenshot if visual issue

2. **Fix the code** following the repository pattern:
   - Repository layer for data issues
   - tRPC route for API issues
   - Component for UI issues

3. **Re-verify** the specific scenario after fixing

## Step 4: Write E2E Test

After manual verification passes, create an e2e test file:

### Test File Location
```
tests/e2e/{feature-name}.spec.ts
```

### E2E Test Template

```typescript
import { test, expect } from "@playwright/test";

test.describe("{Feature Name}", () => {
  test.beforeEach(async ({ page }) => {
    // Setup: login, navigate to starting point
    await page.goto("/login");
    await page.fill('[data-testid="email"]', "test@example.com");
    await page.fill('[data-testid="password"]', "password");
    await page.click('[data-testid="login-button"]');
    await page.waitForURL("/dashboard");
  });

  test("should {happy path description}", async ({ page }) => {
    // Arrange
    await page.goto("/{feature-path}");

    // Act
    await page.click('[data-testid="{element}"]');
    await page.fill('[data-testid="{input}"]', "test value");
    await page.click('[data-testid="{submit}"]');

    // Assert
    await expect(page.locator('[data-testid="{result}"]')).toBeVisible();
    await expect(page.locator('[data-testid="{result}"]')).toContainText("expected text");
  });

  test("should handle {edge case}", async ({ page }) => {
    // Test edge case scenario
  });

  test("should show error when {error condition}", async ({ page }) => {
    // Test error handling
  });
});
```

### Data-TestId Convention

Add `data-testid` attributes to components for reliable test selectors:

```tsx
// Button
<Button data-testid="submit-form">Submit</Button>

// Input
<Input data-testid="search-input" />

// Table row
<TableRow data-testid={`row-${item.id}`}>

// Modal
<Dialog data-testid="confirm-modal">

// Form
<form data-testid="edit-profile-form">
```

### Test Categories

1. **Smoke Tests** - Critical paths that must always work
2. **Feature Tests** - Complete feature workflows
3. **Regression Tests** - Previously broken scenarios
4. **Edge Case Tests** - Boundary conditions and unusual inputs

## Step 5: Create Test Results Document

After completing all verification, create a results document with screenshots and explanations:

### Results File Location
```
.cursor/testing-results/{feature-name}-results.md
```

### Test Results Template

```markdown
# Test Results: {Feature Name}

**Date:** {YYYY-MM-DD}
**Tester:** AI Agent
**Status:** ✅ Passed / ⚠️ Passed with fixes / ❌ Failed

---

## Summary

{Brief summary of what was tested and the overall outcome}

- **Total Scenarios:** {X}
- **Passed:** {X}
- **Fixed During Testing:** {X}
- **Known Issues:** {X}

---

## Test Results by Scenario

### Scenario 1: {Scenario Name}
**Status:** ✅ Passed

**What was tested:**
{Explanation of what this scenario verifies and why it matters}

**Steps performed:**
1. {Step description}
2. {Step description}
3. {Step description}

**Screenshot:**
![{Description}](./screenshots/{feature-name}-scenario-1.png)

**Observations:**
- {What was observed}
- {Any notable behavior}

---

### Scenario 2: {Scenario Name}
**Status:** ⚠️ Fixed

**What was tested:**
{Explanation of what this scenario verifies}

**Issue Found:**
{Description of the issue that was discovered}

**Fix Applied:**
{Description of the code change made to fix the issue}
- File: `{path/to/file.tsx}`
- Change: {Brief description}

**Screenshot (After Fix):**
![{Description}](./screenshots/{feature-name}-scenario-2-fixed.png)

---

### Scenario 3: {Scenario Name}
**Status:** ✅ Passed

**What was tested:**
{Explanation - e.g., "Verifies that users see an error message when submitting invalid data"}

**Screenshot:**
![{Description}](./screenshots/{feature-name}-scenario-3.png)

---

## UI Verification

| Element | Status | Notes |
|---------|--------|-------|
| {Component/Element} | ✅ | Renders correctly |
| {Loading state} | ✅ | Shows spinner during fetch |
| {Empty state} | ✅ | Displays helpful message |
| {Error state} | ✅ | Shows error with retry option |

---

## Issues Fixed During Testing

### Issue 1: {Issue Title}
- **File:** `{path/to/file}`
- **Problem:** {Description}
- **Solution:** {What was changed}

### Issue 2: {Issue Title}
- **File:** `{path/to/file}`
- **Problem:** {Description}
- **Solution:** {What was changed}

---

## E2E Test Coverage

Test file created: `tests/e2e/{feature-name}.spec.ts`

| Test Case | Description |
|-----------|-------------|
| `should {test name}` | {What it verifies} |
| `should {test name}` | {What it verifies} |
| `should {test name}` | {What it verifies} |

---

## Recommendations

{Any recommendations for future improvements or things to watch out for}

---

## Screenshots Directory

All screenshots saved to: `.cursor/testing-results/screenshots/`
```

### Screenshot Naming Convention

Save screenshots with descriptive names:
```
{feature-name}-{scenario}-{state}.png

Examples:
- client-profile-initial-load.png
- client-profile-edit-modal-open.png
- client-profile-save-success.png
- client-profile-validation-error.png
```

### Taking Screenshots During Testing

Use `browser_take_screenshot` at key moments:

```typescript
// After page load
mcp_cursor-ide-browser_browser_take_screenshot({
  filename: "feature-name-initial-load.png"
})

// After action
mcp_cursor-ide-browser_browser_take_screenshot({
  filename: "feature-name-after-submit.png"
})

// Error state
mcp_cursor-ide-browser_browser_take_screenshot({
  filename: "feature-name-error-state.png"
})

// Full page capture
mcp_cursor-ide-browser_browser_take_screenshot({
  filename: "feature-name-full-page.png",
  fullPage: true
})
```

## Checklist

Before marking implementation complete:

- [ ] Testing plan created in `.cursor/testing-plans/`
- [ ] All scenarios manually verified with Playwright MCP
- [ ] Screenshots taken for each scenario
- [ ] Issues found during testing have been fixed
- [ ] E2E test file created in `tests/e2e/`
- [ ] All e2e tests pass locally
- [ ] Data-testid attributes added to key elements
- [ ] Test results document created in `.cursor/testing-results/`

## Quick Reference: Playwright MCP Tools

| Tool | Purpose |
|------|---------|
| `browser_navigate` | Go to URL |
| `browser_snapshot` | Get accessibility tree (best for finding elements) |
| `browser_click` | Click element by ref |
| `browser_type` | Type into input by ref |
| `browser_hover` | Hover over element |
| `browser_select_option` | Select dropdown option |
| `browser_press_key` | Press keyboard key |
| `browser_wait_for` | Wait for text/time |
| `browser_take_screenshot` | Capture visual state |
| `browser_console_messages` | Check for JS errors |
| `browser_network_requests` | Verify API calls |
